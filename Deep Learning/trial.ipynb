{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data_processing import *\n",
    "import scipy.io as spio\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true,y_pred):\n",
    "    C=0\n",
    "    # one-hot encoding\n",
    "    for col in range(y_true.shape[-1]):\n",
    "        y_pred[col] = y_pred[col] if y_pred[col] < 1 else 0.99999\n",
    "        y_pred[col] = y_pred[col] if y_pred[col] > 0 else 0.00001\n",
    "        C+=y_true[col]*torch.log(y_pred[col])+(1-y_true[col])*torch.log(1-y_pred[col])\n",
    "    return -C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39\n",
      "1.38\n",
      "1.38\n",
      "1.38\n",
      "1.38\n",
      "1.38\n",
      "1.38\n",
      "1.38\n",
      "1.37\n",
      "1.37\n",
      "1.37\n",
      "1.37\n",
      "1.37\n",
      "1.37\n",
      "1.37\n",
      "1.36\n",
      "1.36\n",
      "1.36\n",
      "1.36\n",
      "1.36\n",
      "1.36\n",
      "1.35\n",
      "1.35\n",
      "1.35\n",
      "1.35\n",
      "1.35\n",
      "1.35\n",
      "1.34\n",
      "1.34\n",
      "1.34\n",
      "1.34\n",
      "1.34\n",
      "1.34\n",
      "1.33\n",
      "1.33\n",
      "1.33\n",
      "1.33\n",
      "1.33\n",
      "1.33\n",
      "1.33\n",
      "1.33\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.32\n",
      "1.31\n",
      "1.31\n",
      "1.31\n",
      "1.31\n",
      "1.31\n",
      "1.31\n",
      "1.31\n",
      "1.31\n",
      "1.31\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.30\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.29\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n",
      "1.28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "len_traj = 4 \n",
    "batch_size = 16\n",
    "d_obs = 2\n",
    "d_embed = 128 # embedding dimension\n",
    "n_heads = 8\n",
    "d_k = 16\n",
    "d_hidden = 16\n",
    "d_class = 4\n",
    "n_layers = 4 # Encoder内含\n",
    "trajectory = torch.rand(batch_size, len_traj, d_obs)\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    '''将轨迹序列映射到隐空间'''\n",
    "    def __init__(self, inpt_dim, embed_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.fc = nn.Linear(inpt_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)] [1,8,5,5]\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_embed, d_k, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.W_Q = nn.Linear(d_embed, d_k * n_heads) # d_embed,7维, d_k,16*8=128维\n",
    "        self.W_K = nn.Linear(d_embed, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_embed, d_k * n_heads)\n",
    "        self.fc = nn.Linear(n_heads * d_k, d_embed)\n",
    "        self.layer_norm = nn.LayerNorm(d_embed)\n",
    "        self.DotProduct = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = x, x.size(0) # 残差跨层连接\n",
    "        \n",
    "        # q_s = k_s = v_s: [batch_size, n_heads, len_q, d_k]\n",
    "        q_s = self.W_Q(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        k_s = self.W_K(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        v_s = self.W_V(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # context: [batch_size, n_heads, len_q, d_k]\n",
    "        # attn: [batch_size, n_heads, len_q(=len_k), len_k(=len_q)]\n",
    "        context, attn = self.DotProduct(q_s, k_s, v_s) # context是attn✖V\n",
    "        # contiguous()的功能类似deepcopy\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k) # context: [batch_size x len_q x n_heads * d_k] 最后一个维度是将8个head concat起来，维度依然512\n",
    "        \n",
    "        output = self.fc(context) # [batch_size, len_q, d_embed]\n",
    "        return self.layer_norm(output + residual), attn # output: [batch_size, len_q, d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    # 该模块也可用linear+ReLU实现\n",
    "    def __init__(self, d_embed, d_hidden):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_embed, out_channels=d_hidden, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_hidden, out_channels=d_embed, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(d_embed)\n",
    "    def forward(self, x):\n",
    "        residual = x # [batch_size, len_q, d_model]\n",
    "        x = nn.ReLU()(self.conv1(x.transpose(1, 2)))\n",
    "        x = self.conv2(x).transpose(1, 2)\n",
    "        return self.layer_norm(x + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_embed, d_k, n_heads, d_hidden):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.MultiHeadAttention = MultiHeadAttention(d_embed, d_k, n_heads)\n",
    "        self.PoswiseFeedForwardNet = PoswiseFeedForwardNet(d_embed, d_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, attn = self.MultiHeadAttention(x) # x to same Q,K,V\n",
    "        x = self.PoswiseFeedForwardNet(x) # x: [batch_size, len_q, d_embed]\n",
    "        return x, attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    using transformer encoder to classify sequential data\n",
    "    '''\n",
    "    def __init__(self, d_obs, d_embed, d_class, d_k, d_hidden, n_heads, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(inpt_dim=d_obs, embed_dim=d_embed) # state dimension，embedding dimension\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_embed, d_k, n_heads, d_hidden) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_embed, d_class)\n",
    "\n",
    "    def forward(self, x): # enc_inputs : [batch_size x source_len]\n",
    "        y = self.embedding(x)\n",
    "        attentions = []\n",
    "        for layer in self.layers:\n",
    "            y, attention = layer(y)\n",
    "            attentions.append(attention)\n",
    "\n",
    "        out = F.log_softmax(self.fc(y), dim=-1)\n",
    "        return out, attentions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from model.neural_network import LSTM_net\n",
    "    # encoder = Encoder(d_obs, d_embed, d_class, d_k, d_hidden, n_heads, n_layers)\n",
    "    encoder = LSTM_net()\n",
    "    trajectory = torch.rand(batch_size, len_traj, d_obs, dtype=torch.float32)\n",
    "    shape = torch.randint(high = 4, size = (16, 4))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.Adam(\n",
    "        encoder.parameters(),\n",
    "        lr = 0.001,\n",
    "        weight_decay=0.0001\n",
    "    )\n",
    "    for _ in range(100):\n",
    "        pred = encoder(trajectory)\n",
    "        optimiser.zero_grad()\n",
    "        _loss = loss(pred, shape)\n",
    "        _loss.backward()\n",
    "        optimiser.step()\n",
    "        print (f'{_loss.detach():.2f}')\n",
    "\n",
    "    '''\n",
    "    from torchinfo import summary\n",
    "    summary(encoder, (batch_size, len_traj, d_obs))\n",
    "    print(context.shape, attn[0].shape)\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9983, -0.2701,  0.0599]]])\n",
      "tensor([[[-0.6897, -0.0663, -0.4248]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 3)\n",
    "print (x)\n",
    "_x = nn.Linear(3, 3)\n",
    "\n",
    "x = _x(x)\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
